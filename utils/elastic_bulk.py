from elasticsearch import Elasticsearch, helpers
import pandas as pd
import json
import os
from dotenv import load_dotenv
from openai import OpenAI

# .env ë¡œë“œ ë° OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
load_dotenv()
api_key = os.getenv("AI_API_KEY")
client = OpenAI(api_key=api_key)

# OpenAI ì„ë² ë”© ì„¤ì • (rag_moduleê³¼ ë™ì¼í•˜ê²Œ)
EMBEDDING_MODEL = "text-embedding-3-small"
EMBEDDING_DIMS = 1536 

def get_stock_info():
    base_url = "http://kind.krx.co.kr/corpgeneral/corpList.do"    
    method = "download"
    url = f"{base_url}?method={method}"
    df = pd.read_html(url, header=0, encoding='euc-kr')[0]
    df['ì¢…ëª©ì½”ë“œ'] = df['ì¢…ëª©ì½”ë“œ'].apply(lambda x: f"{x:06}")     
    return df

print("KRXì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
df = get_stock_info()

# 1. í…ìŠ¤íŠ¸ ë³‘í•© (í™ˆí˜ì´ì§€ ì œì™¸)
def create_combined_text(row):
    texts = [f"{col}: {row[col]}" for col in df.columns if col != 'í™ˆí˜ì´ì§€' and pd.notna(row[col])]
    return " | ".join(texts)

df['combined_text'] = df.apply(create_combined_text, axis=1)

# 2. OpenAIë¥¼ ì´ìš©í•œ ë²¡í„° ë³€í™˜
def get_embeddings_bulk(text_list):
    print(f"ì´ {len(text_list)}ê°œ ë°ì´í„° ì„ë² ë”© ìƒì„± ì¤‘...")
    # ë¹„ìš© ì ˆì•½ ë° ì†ë„ë¥¼ ìœ„í•´ í•œ ë²ˆì— ìš”ì²­ (Batch)
    response = client.embeddings.create(input=text_list, model=EMBEDDING_MODEL)
    return [data.embedding for data in response.data]

# ë°ì´í„°ê°€ ë§ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ 100ê°œì”© ëŠì–´ì„œ ì„ë² ë”© (API ì œí•œ ë°©ì§€)
all_embeddings = []
batch_size = 100
for i in range(0, len(df), batch_size):
    batch_text = df['combined_text'].iloc[i:i+batch_size].tolist()
    all_embeddings.extend(get_embeddings_bulk(batch_text))

df['embedding'] = all_embeddings

# 3. ì—˜ë¼ìŠ¤í‹±ì„œì¹˜ ì„¤ì •
es = Elasticsearch("http://localhost:9200", http_compress=True)
index_name = 'stock_info'

index_settings = {
    "settings": {
        "index.max_ngram_diff": 3,  # ğŸŒŸ ì´ ì„¤ì •ì„ ì¶”ê°€í•˜ì—¬ ì°¨ì´ê°’ ì œí•œì„ í•´ì œí•©ë‹ˆë‹¤.
        "analysis": {
            "tokenizer": {
                "ngram_tokenizer": {
                    "type": "ngram", 
                    "min_gram": 2, 
                    "max_gram": 5, 
                    "token_chars": ["letter", "digit"]
                }
            },
            "analyzer": {
                "ngram_analyzer": {
                    "type": "custom", 
                    "tokenizer": "ngram_tokenizer"
                }
            }
        }
    },
    "mappings": {
        "properties": {
            "íšŒì‚¬ëª…": {
                "type": "text", 
                "analyzer": "ngram_analyzer",
                "fields": {"keyword": {"type": "keyword"}} 
            },
            "ì—…ì¢…": {"type": "text", "analyzer": "ngram_analyzer"},
            "ì£¼ìš”ì œí’ˆ": {"type": "text", "analyzer": "ngram_analyzer"},
            "ì¢…ëª©ì½”ë“œ": {"type": "keyword"},
            "embedding": {
                "type": "dense_vector",
                "dims": 1536, 
                "index": True,
                "similarity": "cosine" 
            }
        }
    }
}

# ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ í›„ ì¬ìƒì„±
if es.indices.exists(index=index_name):
    es.indices.delete(index=index_name)
es.indices.create(index=index_name, body=index_settings)

# 4. Bulk ì ì¬
json_records = json.loads(df.to_json(orient='records'))
action_list = [
    {
        '_op_type': 'index',
        '_index': index_name,
        '_source': row
    } for row in json_records
]

helpers.bulk(es, action_list)
print("âœ… OpenAI ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° ì ì¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")